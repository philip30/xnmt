import collections
import dynet as dy

from typing import Sequence, Union, Optional, Any

import xnmt
import xnmt.models as models
import xnmt.eval.metrics as metrics
import xnmt.eval.evaluate as evaluate


class LossEvalTask(models.EvalTask, xnmt.Serializable):
  yaml_tag = "!LossEvalTask"
  """
  A task that does evaluation of the loss function.

  Args:
    src_file: source file name
    ref_file: reference file name
    model: generator model to use for inference
    batcher: batcher to use
    loss_calculator: loss calculator
    max_src_len: omit sentences with source length greater than specified number
    max_trg_len: omit sentences with target length greater than specified number
    max_num_sents: compute loss only for the first n sentences in the given corpus
    loss_comb_method: method for combining loss across batch elements ('sum' or 'avg').
    desc: description to pass on to computed score objects
  """
  @xnmt.serializable_init
  def __init__(self,
               src_file: Union[str, Sequence[str]],
               ref_file: Optional[str] = None,
               model: models.ConditionedModel = xnmt.Ref("model"),
               batcher: Optional[xnmt.structs.Batcher] = xnmt.Ref("train.batcher"),
               loss_calculator: models.LossCalculator = xnmt.bare(xnmt.train.MLELoss),
               max_src_len: Optional[int] = None,
               max_trg_len: Optional[int] = None,
               max_num_sents: Optional[int] = None,
               loss_comb_method: str = xnmt.Ref("exp_global.loss_comb_method", default="sum"),
               src_reader: models.InputReader = xnmt.ref_src_reader,
               trg_reader: models.InputReader = xnmt.ref_trg_reader,
               desc: Any = None):
    self.model = model
    self.loss_calculator = loss_calculator
    self.src_file = src_file
    self.ref_file = ref_file
    self.batcher = batcher
    self.src_data = None
    self.max_src_len = max_src_len
    self.max_trg_len = max_trg_len
    self.max_num_sents = max_num_sents
    self.loss_comb_method = loss_comb_method
    self.src_reader = src_reader
    self.trg_reader = trg_reader
    self.desc = desc
    self.src_data = None
    self.ref_data = None
    self.src_batches = None
    self.ref_batches = None

  def eval(self) -> models.EvalScore:
    """
    Perform evaluation task.

    Returns:
      Evaluated score
    """
    xnmt.event_trigger.set_train(False)
    if self.src_data is None:
      self.src_data, self.ref_data, self.src_batches, self.ref_batches = \
        xnmt.modules.input_readers.read_parallel_corpus(src_reader=self.src_reader,
                                           trg_reader=self.trg_reader,
                                           src_file=self.src_file,
                                           trg_file=self.ref_file,
                                           batcher=self.batcher,
                                           max_num_sents=self.max_num_sents,
                                           max_src_len=self.max_src_len,
                                           max_trg_len=self.max_trg_len)
    ref_words_cnt = 0
    loss_maps = collections.defaultdict(float)
    loss_wrds = collections.defaultdict(float)
    for src, trg in zip(self.src_batches, self.ref_batches):
      with xnmt.utils.ReportOnException({"src": src, "trg": trg, "graph": xnmt.utils.print_cg_conditional}):
        dy.renew_cg(immediate_compute=xnmt.settings.IMMEDIATE_COMPUTE, check_validity=xnmt.settings.CHECK_VALIDITY)

        loss_expr = self.loss_calculator.calc_loss(self.model, src, trg)
        loss, loss_value = loss_expr.compute(comb_method=self.loss_comb_method)

        for k, (value, unit) in loss_value.items():
          loss_maps[k] += value
          loss_wrds[k] += unit

    loss_stats = {k: loss_maps[k]/loss_wrds[k] for k in loss_maps.keys()}

    return metrics.LossScore(sum(loss_stats.values()),
                             loss_stats=loss_stats,
                             num_ref_words=ref_words_cnt,
                             desc=self.desc)





class AccuracyEvalTask(models.EvalTask, xnmt.Serializable):
  yaml_tag = "!AccuracyEvalTask"
  """
  A task that does evaluation of some measure of accuracy.

  Args:
    src_file: path(s) to read source file(s) from
    ref_file: path(s) to read reference file(s) from
    hyp_file: path to write hypothesis file to
    model: generator model to generate hypothesis with
    eval_metrics: list of evaluation metrics (list of Evaluator objects or string of comma-separated shortcuts)
    inference: inference object
    perform_inference: Whether to generate the output or not. One eval task can use an already existing hyp_file
                       that was generated by the previous eval tasks.
    desc: human-readable description passed on to resulting score objects
  """
  @xnmt.serializable_init
  def __init__(self,
               src_file: Union[str,Sequence[str]],
               ref_file: Union[str,Sequence[str]],
               hyp_file: str,
               model: Union[models.GeneratorModel, models.Reportable] = xnmt.Ref("model"),
               eval_metrics: Union[str, models.Evaluator, Sequence[models.Evaluator]] = "bleu",
               inference: Optional[models.Inference] = None,
               perform_inference: bool = True,
               desc: Any = None) -> None:
    self.model = model
    if isinstance(eval_metrics, str):
      eval_metrics = [evaluate.eval_shortcuts[shortcut]() for shortcut in eval_metrics.split(",")]
    elif not isinstance(eval_metrics, Sequence): eval_metrics = [eval_metrics]
    self.eval_metrics = eval_metrics
    self.src_file = src_file
    self.ref_file = ref_file
    self.hyp_file = hyp_file
    self.inference = inference
    self.perform_inference = perform_inference
    self.desc = desc

  def eval(self) -> Sequence[models.EvalScore]:
    xnmt.event_trigger.set_train(False)
    if issubclass(self.model.__class__, models.Reportable):
      self.model.report_corpus_info({"ref_file": self.ref_file})
    if self.perform_inference:
      self.inference.perform_inference(generator=self.model,
                                       src_file=self.src_file,
                                       trg_file=self.hyp_file,
                                       ref_file=self.ref_file)
    # Evaluate
    eval_scores = evaluate.evaluate(hyp_file=self.hyp_file,
                                    ref_file=self.ref_file,
                                    desc=self.desc,
                                    evaluators=self.eval_metrics)

    return eval_scores

class DecodingEvalTask(models.EvalTask, xnmt.Serializable):
  yaml_tag = "!DecodingEvalTask"
  """
  A task that does performs decoding without comparing against a reference.

  Args:
    src_file: path(s) to read source file(s) from
    hyp_file: path to write hypothesis file to
    model: generator model to generate hypothesis with
    inference: inference object
  """
  @xnmt.serializable_init
  def __init__(self,
               src_file: Union[str,Sequence[str]],
               hyp_file: str,
               model: models.GeneratorModel = xnmt.Ref("model"),
               inference: Optional[models.Inference] = None):

    self.model = model
    self.src_file = src_file
    self.hyp_file = hyp_file
    self.inference = inference

  def eval(self) -> None:
    xnmt.event_trigger.set_train(False)
    self.inference.perform_inference(generator=self.model,
                                     src_file=self.src_file,
                                     trg_file=self.hyp_file)
    return None
